%\vspace{-1mm} 

Before diving into the description of our contribution, we introduce the state-of-the-art heterogeneous architectures and corresponding programming models for comprehensive performance analysis.

\subsection{Multi-core CPU Architecture}
%\vspace{-4mm} 
  Instead of increasing clock frequencies, the growth of parallelism has become the main purpose of traditional CPUs, which relies strongly on the increase of processor clock speed to increase the ability of computing. A multicore CPU is possible where each core is an independent processor containing multiple parallel pipelines, each pipeline being superscalar. Some processors also include vector capability. In addition, vector processing units have evolved to new instruction sets and longer vector registers. At the same time, modern CPU employs Non-uniform memory access (NUMA), which is a computer memory design used in multiprocessing, where the memory access time depends on the memory location relative to the processor. Under NUMA, CPU can access its own local memory faster than non-local memory (memory local to another processor or memory shared between processors). In addition, the CPU architecture is resource-rich running for task and data parallelism in the applications by using AVX extensions (Advanced Vector Extensions) to quickly operate on floating-point vectors.
  
\subsection{Graphics Processing Units (GPU) Architecture}
%\vspace{-4mm} 
With the requirement and potential of accelerators for high performance computing, Nvidia develops Tesla series that are designed for massively parallel processing and large scale computing, which goes far beyond basic graphics controller functions, and is a programmable and powerful computational accelerator for the high performance computing mark. For the architecture, CPUs are much different from GPUs. CPUs are composed of few cores with lots of cache memory. It handles a few software threads at a time. In contrast, GPUs are composed of hundreds of cores that can handle thousands of threads simultaneously.
  
\subsection{Many Integrated Core (MIC) Architecture}
%  \vspace{-4mm} 
With the growing success of GPUs as a massively parallel processor well suited to more general purpose applications, Intel has introduced a line of products based on a Many Integrated Core (MIC) architecture, marketed under the name Xeon Phi. The Xeon Phi co-processors are symmetric multi-processors; they physically look similar to a GPU in that they plug into a host system via PCI Express (PCIe). They run a Linux Operating System. However, a Xeon Phi coprocessor cannot be used as a stand-alone processor, and requires a host system to operate.
  
From an architectural perspective, they also have many similarities to GPUs. Xeon Phi is based on an x86 Pentium core architecture from the early 1990s. This is a much simplified and hence smaller core compared to modern x86 CPUs. To make these simplified cores more computationally powerful, 512-bit-wide vector units have been added to the core. Because of the simplified nature of the core, each co-processor features 60+ cores clocked at 1 GHz or more, supporting 64-bit x86 instructions. The exact number of cores depends on the model and the generation of the product. These in-order cores support four-way hyper-threading, resulting in more than 240 logical cores. The cores of an Xeon Phi coprocessor are interconnected by a high speed bidirectional ring, which unites the L2 caches of the cores into a large coherent aggregate cache over 30 MB in size. The coprocessor is equipped with 6 to 16 GB of on-board GDDR5 memory. The speed and energy efficiency of Xeon Phi coprocessors come from their vector units. Each core contains a vector-processing unit (VPU) with 512-bit SIMD (Single Instruction Multiple Data) vectors supporting a new instruction set called Intel Initial Many-Core Instructions (Intel IMCI). The Intel IMCI includes, among other instructions the Fused Multiply-Add (FMA), reciprocal, square root, power and exponential function operations, commonly used in physical modeling and statistical analysis\cite{R:22}.
  
\subsection{Programming Models\\}
%\vspace{-4mm} 
The programming tools and languages employed for code development for MICs are the same as those used for CPUs. This is a significant advantage as compared to GPUs that alleviates code migration overhead for the co-processor. For the applications, the implement uses OpenMP for MIC and CPU, and uses CUDA for GPU. MIC supports two execution modes: native and offload. In the native mode the application runs entirely within the co-processor. The offload mode allows for CPU to execute regions of the application code with the co-processor. These regions are defined using pragma tags and include directives for transferring data. The offload mode also supports conditional offload directives, which may be used to decide at runtime whether a region should be offloaded to the co-processor or should be executed on CPU. This feature is used in the applications. Since we execute the codes on MIC using the offload mode, a computing core is reserved to run the offload daemon, and a maximum of 240 computing cores are launched.
  
  


  
